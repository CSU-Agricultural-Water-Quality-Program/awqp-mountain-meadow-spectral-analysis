View(clean.df)
View(cal.data)
package.list <- c("dplyr",
"ggplot2",
"GGally",
"caret"
)
packageLoad <- function(packages){
for (i in packages) {
if (!require(i, character.only = TRUE)) {
install.packages(i)
library(i, character.only = TRUE)
}
}
}
packageLoad(package.list)
# Step 2: Import data
# Step 2a: Import calibration data from .csv file
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
colnames(cal.data)
# Step 3a: Remove columns not needed for analysis
clean.df <- cal.data %>%
select(c(ID, NDVI, NDRE, N.Content....))
colnames(clean.df)
# Step 4: Data exploration via ggplot2 and other methods
# Scatterplot and Pearson's R correlation matrix
ggpairs(clean.df)
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
colnames(cal.data)
# Step 3a: Remove columns not needed for analysis
clean.df <- cal.data %>%
select(c(ID, NDVI, NDRE, N.Content...., N))
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
colnames(cal.data)
# Step 3a: Remove columns not needed for analysis
clean.df <- cal.data %>%
select(c(ID, NDVI, NDRE, N.Content....))
colnames(clean.df)
# Step 4: Data exploration via ggplot2 and other methods
# Scatterplot and Pearson's R correlation matrix
ggpairs(clean.df)
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
colnames(cal.data)
# Step 3a: Remove columns not needed for analysis
clean.df <- cal.data %>%
select(c(Red, Green, Red.Edge, NIR, N.Content....))
colnames(clean.df)
# Step 4: Data exploration via ggplot2 and other methods
# Scatterplot and Pearson's R correlation matrix
ggpairs(clean.df)
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
colnames(cal.data)
# Step 3a: Remove columns not needed for analysis
clean.df <- cal.data %>%
filter(Date == '7/10/2023') %>%
select(c(Red, Green, Red.Edge, NIR, N.Content....))
colnames(clean.df)
# Step 4: Data exploration via ggplot2 and other methods
# Scatterplot and Pearson's R correlation matrix
ggpairs(clean.df)
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
colnames(cal.data)
# Step 3a: Remove columns not needed for analysis
clean.df <- cal.data %>%
filter(Date == '5/31/2023') %>%
select(c(Red, Green, Red.Edge, NIR, N.Content....))
colnames(clean.df)
# Step 4: Data exploration via ggplot2 and other methods
# Scatterplot and Pearson's R correlation matrix
ggpairs(clean.df)
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
colnames(cal.data)
# Step 3a: Remove columns not needed for analysis
clean.df <- cal.data %>%
filter(Date == '7/10/2023') %>%
#select(c(Red, Green, Red.Edge, NIR, N.Content....))
select(c(NDVI, NDRE, N.Content....))
colnames(clean.df)
# Step 4: Data exploration via ggplot2 and other methods
# Scatterplot and Pearson's R correlation matrix
ggpairs(clean.df)
clean.df$N <- clean.df$NO3_D1_ppm # soil NO3, ppm
#clean.df$N <- clean.df$N_kg.ha.1 # plant N, kg/ha
#clean.df$N <- clean.df$kg.ha.1 # plant biomass, g/ft^2?
# Step 5a: Linear regression
lm.mdl <- lm(N~NDVI, data=clean.df)
clean.df$N <- clean.df$NO3_D1_ppm # soil NO3, ppm
#clean.df$N <- clean.df$N_kg.ha.1 # plant N, kg/ha
#clean.df$N <- clean.df$kg.ha.1 # plant biomass, g/ft^2?
# Step 5a: Linear regression
lm.mdl <- lm(N~NDVI, data=clean.df)
clean.data <- cal.data
clean.data$N_lbs <- (cal.data$N.Content..mg./453592.37)
View(clean.data)
clean.data$N_lbs_ac <- (clean.data$N_lbs*43560)
View(clean.data)
clean.data$N_perc_ac <- (cal.data$N.Content....*43560)
View(clean.data)
clean.data$N_perc <- (cal.data$N.Content....)
package.list <- c("dplyr",
"ggplot2",
"GGally",
"caret"
)
packageLoad <- function(packages){
for (i in packages) {
if (!require(i, character.only = TRUE)) {
install.packages(i)
library(i, character.only = TRUE)
}
}
}
packageLoad(package.list)
# Step 2: Import data
# Step 2a: Import calibration data from .csv file
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
clean.data <- cal.data
clean.data$N_lbs <- (cal.data$N.Content..mg./453592.37)
clean.data$N_lbs_ac <- (clean.data$N_lbs*43560)
clean.data$N_perc <- (cal.data$N.Content....)
View(clean.data)
sub.df <- clean.data %>%
filter(Date == '7/10/2023') %>%
#select(c(Red, Green, Red.Edge, NIR, N.Content....))
select(c(NDVI, NDRE, N.Content....))
colnames(sub.df)
# Scatterplot and Pearson's R correlation matrix
ggpairs(sub.df)
# Using regression to predict plan and soil N status in mountain meadow hay
# systems using spectral data derived from drone imagery.
# Created by A.J. Brown
# Date created: 5 July 2023
# Date last modified: 6 July 2023
# Script work flow:
# Step 1: Import libraries
# Step 2: Import data
# Step 3: Data cleaning
# Step 4: Data exploration via ggplot2 and other methods
# Step 5: Regression model creation
# Step 6: Model goodness of fit analysis
# Step 7: Model selection, final prediction, and data export
# Step 8: use final model to interpolate N status across the study area
# Step 9: export final interpolated data
# Step 1: Import libraries
package.list <- c("dplyr",
"ggplot2",
"GGally",
"caret"
)
packageLoad <- function(packages){
for (i in packages) {
if (!require(i, character.only = TRUE)) {
install.packages(i)
library(i, character.only = TRUE)
}
}
}
packageLoad(package.list)
# Step 2: Import data
# Step 2a: Import calibration data from .csv file
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
clean.df <- cal.data
clean.df$N_lbs <- (cal.data$N.Content..mg./453592.37)
clean.df$N_lbs_ac <- (clean.df$N_lbs*43560)
clean.df$N_perc <- (cal.data$N.Content....)
# Step 4: Data exploration via ggplot2 and other methods\
# Step 4a: Remove columns not needed for analysis
sub.df <- clean.df %>%
filter(Date == '7/10/2023') %>%
#select(c(Red, Green, Red.Edge, NIR, N.Content....))
select(c(NDVI, NDRE, N.Content....))
colnames(sub.df)
# Scatterplot and Pearson's R correlation matrix
ggpairs(sub.df)
# Step 5: Regression model creation
# Set dependent variable for prediction (uncomment for each model)
clean.df$N <- clean.df$N.Content.... # Plant N (mg/mg)
#clean.df$N <- clean.df$N_kg.ha.1 # plant N, kg/ha
#clean.df$N <- clean.df$kg.ha.1 # plant biomass, g/ft^2?
# Step 5a: Linear regression
lm.mdl <- lm(N~NDVI, data=clean.df)
summary(lm.mdl)
# Confidence Intervals
confint(lm.mdl, level = 0.95)
# Diagnostic plots
par(mfrow=c(2,2))
plot(lm.mdl)
# Visualize model fit
ggplot(data=clean.df, aes(NDVI, N)) +
geom_point() +
geom_smooth(method='lm') +
ggtitle("Linear Regression Model") +
xlab(expression(NDVI)) +
ylab("N (mg/mg)") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw()
# Step 5b: Multiple regession
mult.mdl <- lm(N~NDRE*NDVI, data=clean.df)
summary(mult.mdl)
# Confidence Intervals
confint(mult.mdl, level = 0.95)
# Diagnostic plots
par(mfrow=c(2,2))
plot(mult.mdl)
# Step 5c: Non-linear regression
# Step 6: Model goodness of fit (GOF) analysis
# Step 6a: create GOF functions
# 1:1 plot
oneToOne <- function(pred, obs, data) {
ggplot(data = data, aes(pred, obs)) +
geom_point() +
geom_abline(slope = 1) +
ggtitle("1:1 Plot of Observed and Model Predicted Values") +
xlab(expression(Observed~N~mg~kg^{-1})) +
ylab(expression(Predicted~N~mg~kg^{-1})) +
theme(plot.title = element_text(hjust = 0.5))
}
# RMSE - not needed, as caret includes this fxn
# repeated k-folds cross validation RMSE fxn
kFold <- function(df,
best.model,
model.method="lm",
folds = 10,
repeats = 1000) {
# Set seed for reproducibility
set.seed(123)
# Set up repeated k-fold cross validation paremeters
train.control = trainControl(method = "repeatedcv",
number = folds,
repeats = repeats)
# Pass CV parameters to train function in caret
model = train(best.model$formula,
data = df,
method = model.method,
trControl = train.control
)
# Summarize and print the results
print(model)
}
# Step 6b: GOF analysis for linear regression
clean.df$lm.pred <- predict(lm.mdl)
View(sub.df)
package.list <- c("dplyr",
"ggplot2",
"GGally",
"caret"
)
packageLoad <- function(packages){
for (i in packages) {
if (!require(i, character.only = TRUE)) {
install.packages(i)
library(i, character.only = TRUE)
}
}
}
packageLoad(package.list)
# Step 2: Import data
# Step 2a: Import calibration data from .csv file
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
clean.df <- cal.data %>%
select(Date, NDVI, NDRE, Red, Green, Red.Edge, NIR, NO3_D1_ppm) %>%
filter(Date == "7/10/2023")
clean.df$N_lbs <- (cal.data$N.Content..mg./453592.37)
package.list <- c("dplyr",
"ggplot2",
"GGally",
"caret"
)
packageLoad <- function(packages){
for (i in packages) {
if (!require(i, character.only = TRUE)) {
install.packages(i)
library(i, character.only = TRUE)
}
}
}
packageLoad(package.list)
# Step 2: Import data
# Step 2a: Import calibration data from .csv file
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
clean.df <- cal.data %>%
mutate(N_lbs = N.Content..mg./453592.37,
N_lbs_ac = N_lbs*43560,
N_perc = N.Content....
) %>%
select(Date, NDVI, NDRE, Red, Green, Red.Edge, NIR, NO3_D1_ppm, N_lbs, N_lbs_ac, N_perc
) %>%
filter(Date == '7/10/2023')
# Step 4: Data exploration via ggplot2 and other methods\
# Step 4a: Remove columns not needed for analysis
sub.df <- clean.df %>%
filter(Date == '7/10/2023') %>%
#select(c(Red, Green, Red.Edge, NIR, N.Content....))
select(c(NDVI, NDRE, N.Content....))
cal.file.path <- "./Yampa 2023 Data/Final Sample points with NDVI and NDRE.csv"
cal.data <- read.csv(cal.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 2b: Import interpolation data from .csv file
int.file.path <- "./Yampa 2023 Data/ndvi_ndre_utm_5m.csv"
int.data <- read.csv(int.file.path,
header = TRUE,
sep = ",",
na.strings = c("N/A", " ", "")
)
# Step 3: Data cleaning (if needed)
clean.df <- cal.data %>%
mutate(N_lbs = N.Content..mg./453592.37,
N_lbs_ac = N_lbs*43560,
N_perc = N.Content....
) %>%
select(Date, NDVI, NDRE, Red, Green, Red.Edge, NIR, NO3_D1_ppm, N_lbs, N_lbs_ac, N_perc
) %>%
filter(Date == '7/10/2023')
# Step 4: Data exploration via ggplot2 and other methods\
# Step 4a: Remove columns not needed for analysis
sub.df <- clean.df %>%
filter(Date == '7/10/2023') %>%
#select(c(Red, Green, Red.Edge, NIR, N.Content....))
select(c(NDVI, NDRE, N_perc))
colnames(sub.df)
# Scatterplot and Pearson's R correlation matrix
ggpairs(sub.df)
# Step 5: Regression model creation
# Set dependent variable for prediction (uncomment for each model)
clean.df$N <- clean.df$N_perc # Plant N (mg/mg)
#clean.df$N <- clean.df$N_kg.ha.1 # plant N, kg/ha
#clean.df$N <- clean.df$kg.ha.1 # plant biomass, g/ft^2?
# Step 5a: Linear regression
lm.mdl <- lm(N~NDVI, data=clean.df)
summary(lm.mdl)
# Confidence Intervals
confint(lm.mdl, level = 0.95)
# Diagnostic plots
par(mfrow=c(2,2))
plot(lm.mdl)
# Visualize model fit
ggplot(data=clean.df, aes(NDVI, N)) +
geom_point() +
geom_smooth(method='lm') +
ggtitle("Linear Regression Model") +
xlab(expression(NDVI)) +
ylab("N (mg/mg)") +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw()
mult.mdl <- lm(N~NDRE*NDVI*Red*Green*Red.Edge*NIR, data=clean.df)
summary(mult.mdl)
# Confidence Intervals
confint(mult.mdl, level = 0.95)
# Diagnostic plots
par(mfrow=c(2,2))
plot(mult.mdl)
View(clean.df)
mult.mdl <- lm(N~NDVI*Red*Green, data=clean.df)
summary(mult.mdl)
# Confidence Intervals
confint(mult.mdl, level = 0.95)
# Diagnostic plots
par(mfrow=c(2,2))
plot(mult.mdl)
mult.mdl <- lm(N~NDVI*Green, data=clean.df)
summary(mult.mdl)
# Confidence Intervals
confint(mult.mdl, level = 0.95)
# Diagnostic plots
par(mfrow=c(2,2))
plot(mult.mdl)
